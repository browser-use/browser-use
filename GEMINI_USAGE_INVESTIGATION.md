# Gemini Usage Storage Investigation

## Question
> "For gemini in chat google could it be that we dont store the usage correctly all the time?"

**Answer: YES, there is a potential edge case where usage might not be stored.**

## Analysis Summary

### Current Implementation ‚úÖ

The current implementation in `browser_use/llm/google/chat.py` **correctly** includes usage in all return paths:

1. **Text responses** (line 244-249): ‚úÖ Usage is extracted and included
2. **Structured output with native JSON** (lines 272-322): ‚úÖ Usage is extracted once and included in all return statements
3. **Fallback JSON mode** (lines 353-372): ‚úÖ Usage is extracted and included

### The Potential Issue ‚ö†Ô∏è

The `_get_usage()` method (lines 142-164) has a condition:

```python
def _get_usage(self, response: types.GenerateContentResponse) -> ChatInvokeUsage | None:
    usage: ChatInvokeUsage | None = None
    
    if response.usage_metadata is not None:  # <-- This is the key check
        # ... extract usage ...
        usage = ChatInvokeUsage(...)
    
    return usage  # Returns None if usage_metadata is None
```

**If `response.usage_metadata` is `None`, the function returns `None`, and no usage will be tracked.**

### When Does This Happen?

According to Google's Gemini API documentation, `usage_metadata` should always be present in successful responses. However, there are potential edge cases:

1. **Error responses** - But these throw exceptions before reaching usage extraction
2. **Rate-limited responses** - May have partial metadata
3. **Certain model configurations** - Some settings might suppress usage metadata
4. **Early API responses** - During initial processing, metadata might not be ready
5. **Cached responses** - Fully cached responses might have different metadata structure

### Token Tracking Flow

When usage is `None`, it's not tracked:

```python
# In TokenCost.register_llm() wrapper (tokens/service.py:325-326)
if result.usage:  # <-- This check fails when usage is None
    usage = token_cost_service.add_usage(llm.model, result.usage)
```

## Recommendations

### 1. Add Defensive Logging

Add a warning when usage_metadata is None:

```python
def _get_usage(self, response: types.GenerateContentResponse) -> ChatInvokeUsage | None:
    usage: ChatInvokeUsage | None = None
    
    if response.usage_metadata is not None:
        # ... existing code ...
    else:
        self.logger.warning(f'‚ö†Ô∏è No usage_metadata in response from {self.model}')
    
    return usage
```

### 2. Add Telemetry

Track how often usage is None to understand if this is a real issue:

```python
if usage is None:
    self.logger.debug(f'üìä Missing usage metadata - Model: {self.model}, Response: {response}')
```

### 3. Check Response Object

Investigate what other fields are available in the response when usage_metadata is None:

```python
if response.usage_metadata is None:
    self.logger.debug(f'Response fields: {dir(response)}')
    self.logger.debug(f'Response attributes: {vars(response)}')
```

### 4. Add Tests

Create tests to verify usage is captured in all scenarios:
- Normal text responses
- Structured output responses  
- Responses with thinking budget
- Responses with images
- Large context responses
- Error recovery scenarios

## Conclusion

**YES, there is a potential issue** where usage might not be stored if `response.usage_metadata` is `None`. 

However, based on the API documentation, this should be **rare** and only happen in edge cases. The current implementation is **correct** for normal operation, but lacked:

1. ~~**Logging** to identify when usage is missing~~ ‚úÖ **FIXED** - Added warning log
2. **Telemetry** to measure how often this occurs (future improvement)
3. **Tests** to verify usage tracking in all scenarios ‚úÖ **EXISTS** - `test_single_step.py` tests usage tracking
4. **Documentation** about this edge case ‚úÖ **ADDED** - This document

## Changes Made

### 1. Added Defensive Logging ‚úÖ

Modified `browser_use/llm/google/chat.py` line 163-169:

```python
else:
    # Log when usage metadata is missing to help identify edge cases
    self.logger.warning(
        f'‚ö†Ô∏è  No usage_metadata in response from {self.model}. '
        f'Usage tracking will be skipped for this request. '
        f'This may occur with certain API errors or rate limits.'
    )
```

This will help identify when and how often usage metadata is missing, allowing us to:
- Monitor if this is a real issue in production
- Debug specific scenarios where it occurs
- Gather data to improve handling

### 2. Verified Existing Tests ‚úÖ

Found existing tests that verify usage tracking:
- `browser_use/llm/tests/test_single_step.py` (line 137-138)
- Tests Google Gemini with `gemini-2.0-flash-exp`
- Asserts that `response.usage` is not None and has tokens

## Next Steps

1. **Monitor logs** - Watch for the warning message in production/testing
2. **Investigate patterns** - If warnings appear, investigate what triggers them
3. **Add handling** - If it's a real issue, add fallback logic to estimate usage
4. **Update telemetry** - Add metrics to track usage capture rate per model