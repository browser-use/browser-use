---
title: "Supported Models"
description: "Choose your favorite LLM"
icon: "robot"

---

### Recommendations

- Best accuracy: `O3`
- Fastest: `llama4` on groq
- Balanced: fast + cheap + clever: `gemini-flash-latest` or `gpt-4.1-mini`


### OpenAI [example](https://github.com/browser-use/browser-use/blob/main/examples/models/gpt-4.1.py)

`O3` model is recommended for best performance.

```python
from browser_use import Agent, ChatOpenAI

# Initialize the model
llm = ChatOpenAI(
    model="o3",
)

# Create agent with the model
agent = Agent(
    task="...", # Your task here
    llm=llm
)
```

Required environment variables:

```bash .env
OPENAI_API_KEY=
```

<Info>
  You can use any OpenAI compatible model by passing the model name to the
  `ChatOpenAI` class using a custom URL (or any other parameter that would go
  into the normal OpenAI API call).
</Info>

### Anthropic [example](https://github.com/browser-use/browser-use/blob/main/examples/models/claude-4-sonnet.py)

```python
from browser_use import Agent, ChatAnthropic

# Initialize the model
llm = ChatAnthropic(
    model="claude-sonnet-4-0",
)

# Create agent with the model
agent = Agent(
    task="...", # Your task here
    llm=llm
)
```

And add the variable:

```bash .env
ANTHROPIC_API_KEY=
```

### Azure OpenAI [example](https://github.com/browser-use/browser-use/blob/main/examples/models/azure_openai.py)

```python
from browser_use import Agent, ChatAzureOpenAI
from pydantic import SecretStr
import os

# Initialize the model
llm = ChatAzureOpenAI(
    model="o4-mini",
)

# Create agent with the model
agent = Agent(
    task="...", # Your task here
    llm=llm
)
```

Required environment variables:

```bash .env
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_KEY=
```

### Gemini [example](https://github.com/browser-use/browser-use/blob/main/examples/models/gemini.py)

> [!IMPORTANT] `GEMINI_API_KEY` was the old environment var name, it should be called `GOOGLE_API_KEY` as of 2025-05.

```python
from browser_use import Agent, ChatGoogle
from dotenv import load_dotenv

# Read GOOGLE_API_KEY into env
load_dotenv()

# Initialize the model
llm = ChatGoogle(model='gemini-flash-latest')

# Create agent with the model
agent = Agent(
    task="Your task here",
    llm=llm
)
```

#### Google Search Grounding [example](https://github.com/browser-use/browser-use/blob/main/examples/models/gemini_google_search.py)

Enable Google Search grounding for **direct LLM calls** to get real-time information instantly:

```python
from browser_use import ChatGoogle
from browser_use.llm.messages import UserMessage
from dotenv import load_dotenv

load_dotenv()

# Enable Google Search grounding for direct LLM calls
llm = ChatGoogle(
    model='gemini-2.5-flash',
    google_search=True  # Enables Google Search grounding
)

# Direct LLM call with current information
response = await llm.ainvoke(
    messages=[UserMessage(content="Who won Euro 2024?")]
)

print(response.completion)  # Current, accurate information
print(response.grounding_metadata)  # Sources and search queries used
```

<Warning>
For browser automation agents, Google Search grounding is disabled by default since agents have their own browser-based search tools. Use Google Search grounding primarily for direct LLM calls when you need instant access to current information.
</Warning>

#### Google Search + Structured Output [example](https://github.com/browser-use/browser-use/blob/main/examples/models/gemini_google_search_structured.py)

Google Search grounding works seamlessly with structured output for precise, current data:

```python
from browser_use import ChatGoogle
from pydantic import BaseModel, Field

class MatchResult(BaseModel):
    winner: str = Field(description="The team that won")
    final_score: str = Field(description="The final score")
    venue: str = Field(description="Stadium where final was played")

# Combine Google Search with structured output
llm = ChatGoogle(
    model='gemini-2.5-flash',
    google_search=True  # Real-time information + structured response
)

# Get current information in a structured format
response = await llm.ainvoke(
    messages=[{"role": "user", "content": "Who won Euro 2024?"}],
    output_format=MatchResult
)

print(f"Winner: {response.completion.winner}")
print(f"Score: {response.completion.final_score}")
```

<Tip>
Google Search grounding automatically provides the model with access to current information from Google Search. This is particularly useful for tasks requiring up-to-date facts, current events, or recent developments. It works perfectly with structured output to get current data in your desired format.
</Tip>

Required environment variables:

```bash .env
GOOGLE_API_KEY=
```

### AWS Bedrock [example](https://github.com/browser-use/browser-use/blob/main/examples/models/aws.py)

AWS Bedrock provides access to multiple model providers through a single API. We support both a general AWS Bedrock client and provider-specific convenience classes.

#### General AWS Bedrock (supports all providers)

```python
from browser_use import Agent, ChatAWSBedrock

# Works with any Bedrock model (Anthropic, Meta, AI21, etc.)
llm = ChatAWSBedrock(
    model="anthropic.claude-3-5-sonnet-20240620-v1:0",  # or any Bedrock model
    aws_region="us-east-1",
)

# Create agent with the model
agent = Agent(
    task="Your task here",
    llm=llm
)
```

#### Anthropic Claude via AWS Bedrock (convenience class)

```python
from browser_use import Agent, ChatAnthropicBedrock

# Anthropic-specific class with Claude defaults
llm = ChatAnthropicBedrock(
    model="anthropic.claude-3-5-sonnet-20240620-v1:0",
    aws_region="us-east-1",
)

# Create agent with the model
agent = Agent(
    task="Your task here",
    llm=llm
)
```

#### AWS Authentication

Required environment variables:

```bash .env
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=us-east-1
```

You can also use AWS profiles or IAM roles instead of environment variables. The implementation supports:

- Environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_DEFAULT_REGION`)
- AWS profiles and credential files
- IAM roles (when running on EC2)
- Session tokens for temporary credentials
- AWS SSO authentication (`aws_sso_auth=True`)

## Groq [example](https://github.com/browser-use/browser-use/blob/main/examples/models/llama4-groq.py)

```python
from browser_use import Agent, ChatGroq

llm = ChatGroq(model="meta-llama/llama-4-maverick-17b-128e-instruct")

agent = Agent(
    task="Your task here",
    llm=llm
)
```

Required environment variables:

```bash .env
GROQ_API_KEY=
```

## Ollama

1. Install Ollama: https://github.com/ollama/ollama
2. Run `ollama serve` to start the server 
3. In a new terminal, install the model you want to use: `ollama pull llama3.1:8b` (this has 4.9GB)

```python
from browser_use import Agent, ChatOllama

llm = ChatOllama(model="llama3.1:8b")
```

## Langchain

[Example](https://github.com/browser-use/browser-use/blob/main/examples/models/langchain) on how to use Langchain with Browser Use.

## Qwen [example](https://github.com/browser-use/browser-use/blob/main/examples/models/qwen.py)

Currently, only `qwen-vl-max` is recommended for Browser Use. Other Qwen models, including `qwen-max`, have issues with the action schema format.
Smaller Qwen models may return incorrect action schema formats (e.g., `actions: [{"go_to_url": "google.com"}]` instead of `[{"go_to_url": {"url": "google.com"}}]`). If you want to use other models, add concrete examples of the correct action format to your prompt.

```python
from browser_use import Agent, ChatOpenAI
from dotenv import load_dotenv
import os

load_dotenv()

# Get API key from https://modelstudio.console.alibabacloud.com/?tab=playground#/api-key
api_key = os.getenv('ALIBABA_CLOUD')
base_url = 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1'

llm = ChatOpenAI(model='qwen-vl-max', api_key=api_key, base_url=base_url)

agent = Agent(
    task="Your task here",
    llm=llm,
    use_vision=True
)
```

Required environment variables:

```bash .env
ALIBABA_CLOUD=
```

## ModelScope [example](https://github.com/browser-use/browser-use/blob/main/examples/models/modelscope_example.py)

```python
from browser_use import Agent, ChatOpenAI
from dotenv import load_dotenv
import os

load_dotenv()

# Get API key from https://www.modelscope.cn/docs/model-service/API-Inference/intro
api_key = os.getenv('MODELSCOPE_API_KEY')
base_url = 'https://api-inference.modelscope.cn/v1/'

llm = ChatOpenAI(model='Qwen/Qwen2.5-VL-72B-Instruct', api_key=api_key, base_url=base_url)

agent = Agent(
    task="Your task here",
    llm=llm,
    use_vision=True
)
```

Required environment variables:

```bash .env
MODELSCOPE_API_KEY=
```

## Other models (DeepSeek, Novita, X...)

We support all other models that can be called via OpenAI compatible API. We are open to PRs for more providers.

**Examples available:**
- [DeepSeek](https://github.com/browser-use/browser-use/blob/main/examples/models/deepseek-chat.py)
- [Novita](https://github.com/browser-use/browser-use/blob/main/examples/models/novita.py)
- [OpenRouter](https://github.com/browser-use/browser-use/blob/main/examples/models/openrouter.py)
